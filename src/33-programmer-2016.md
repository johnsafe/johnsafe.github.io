## 乐视商城抢购系统深度实践

文 / 夏智卿

乐视商城的业务量在经历了一次又一次爆发式增长后，技术也面临着新的挑战，技术调整和架构升级成为了工作重点方向，本文主要分享了乐视商城的技术人员在这方面的深度实践经验。

### 前言
销售奇迹的到来，让乐视商城线上系统承受着一次又一次的访问高峰，大促时经受了几百亿次访问。日志分析系统汇总到抢购系统下面的访问记录：
- 乐视商城抢购高峰曾经半小时内有超过100亿次访问，全天超过200亿次访问；
- 无序列表每分钟有超过5000万次访问请求提交到抢购 URL 地址；
- 抢购系统每分钟要处理超过2000万次抢购请求。

这是对一个电商平台的大并发能力技术大考，为了实现突破，技术人员经历过好多的不眠之夜，付出过几多艰辛，冲破一系列技术难题，在这里为大家分享一下乐视商城在这方面的深度实践经验。
### 构筑多层次的防刷单体系
虽然抢购量相对于我们系统容量来说一分钟内就可以轻松处理完成，但那只存在于“网络理想国”之内，真实的网络抢购要面对的却是刷单流量大于正常抢购流量。每一次抢购面对的都是网络攻击如洪水猛兽、刷单手法诡异多变、不明势力频放冷枪，单靠应用程序来应对挑战很容易被攻陷，因此必须建立全方位的防御机制和体系，构筑多层次、多维度、多领域网络“马奇诺防线”。运维小二们基于两年来大大小小几十次交手经验和惨痛教训，在之前就开始着手构建较为完善的网络防护体系，小试牛刀之后，集全集团所有网络大拿之大成梳理出一套较为成熟得防控机制。

面对网络攻击流量的不同大小，采用不同的防护机制。我们为交易关键节点配置了防 DDOS（就是海量流量攻击，通常T级别的，相当于几万个家庭上网带宽）防火墙，通过 WAF 在外网入口层面做好攻击防护。中等流量通过限制其并发访问量来进行约束，在外层 Ngnix 进行 IP 并发连接数限制，这个数字不能太保守，否则容易把通过网关或代理出来的用户给误拦截，但单个 IP 超过1000个并发显然是不合理的。散兵游勇似的小流量攻击或刷单则需要通过内层商业版 Ngnix 对于单个 IP 的 URL 访问频次来限制，把流量相对均衡地分配给不同的来源 IP，防止刷单行为抢占普通用户的抢购通道。

服务器集群负载均衡在网络攻击面前是个伪命题，为了防止个别服务器访问量过大而瘫痪，我们配置了流量管道和排队机制，当然就是启用了 Ngnix 的粗细管道功能，对于单台服务器同时能够同时处理多少个请求设置相应的上限，超过上限的请求就会转给其他服务器来处理；当所有服务器都达到处理上限时，会触发排队机制，访问请求会被放到请求缓冲池中，待上一批次处理完了再依次从访问缓冲池中拿出并处理；我们通常建议队列深度为细管道的5-10倍，细管道超时时间要设为压测时平均响应时间的1.5倍左右，既防止高负载时大批用户被强制超时返回又能保证服务队列请求不至于等待时间过长。

对于那些使用刷单程序，也做了相应的限制，乐视自己研发的防刷单系统会有效识别和过滤那些刷单行为，对其访问予以拒绝。全方位监控平台和实时大数据分析系统可以第一时间掌握系统运行情况，在几十秒内就可以分析出各种恶意攻击行为，变被动为主动积极应战。

<img src="http://ipad-cms.csdn.net/cms/attachment/201608/579ef8fb98b45.jpg" alt="图1  多层次防刷单体系规则体系" title="图1  多层次防刷单体系规则体系" />

图1  多层次防刷单体系规则体系

### 基于 LUA 的防刷单系统
普通 J2EE 程序在单应用实例上性能达到1万 TPS 已经是很高了，防刷单系统如果用 Uowa 实现，必须要有比抢购系统还多的服务节点才能保证自身性能，以保证在大流量时不首先发生崩溃。因此，防刷单系统基于 OpenResty+Ngnix 高性能框架，用 Lua 脚本实现，单台性能可以达到25万 TPS。

防刷单系统通过过滤抢购 HTTP 请求参数，识别出非页面跳转请求和刷单用户，拒绝其继续访问，返回未抢购成功页面。刷单用户清单来源于大数据平台对于交易行为分析的结果，定时更新 Redis 库。Lua 程序将黑名单用户和刷单规则缓存在本地内存，每20秒 Redis 规则库中更新一次，保证高并发下的 Ngnix 从本地内存获取规则信息。
### 抢购交易原子性的实现逻辑
抢购商品库存和单用户抢购次数在高并发时不可能采用传统的查询-判断-修改的处理逻辑，在程序判断周期内数据已经被修改，查询回来的数量是脏数据，再在这个数据基础上去修改是不可以的。业界有很多种处理原子性的技术方案，比如悲观锁、乐观锁、队列等，但性能都无法满足要求，同时对程序逻辑侵入性比较大，容易触发系统崩溃。我们在综合考虑各种方案后，决定借助 Redis 天然的原子性处理机制，将商品库存量和用户抢购次数以 Key-Value 形式存在 Redis 集群中，每次符合购买资格的用户的抢购请求直接对数量进行-1处理，并把 Redis 的 DECR 返回值是否大于等于0作为是否购买成功的判断标准，这样每秒可以轻松达到20万 TPS。这种方案还为我们提供了抢购热度的计量，每次抢购库存都会产生负值，这正是没有抢到的用户量，仅在乐2Max 顶配款秒罄时就产生了负8万多的库存值。
### 高并发下的存储平台改造
购物车作为电商交易入口，在业界来说通常要承担成交订单3到15倍的单量，是高并发访问与业务规则处理并举的功能点，之前通过不断扩容数据库节点来实现，但要达到每秒百万级底层访问速率，硬件及管理成本高到吓人，访问速度也无法提升到毫秒级。先是尝试了 Codis 读写分离的方案，但购物车是高事务一致性的业务，业务高峰出现读库同步信息延时是一个无法逾越的坎，同时购物车读写量趋近，也无法有效发挥多读优势，无奈只好放弃。随后又用 Redis 3.0 Cluster 做了尝试，研究其分布式原理是随机接收访问请求，发现本节点没有再跳转到相应节点，这种方案对于大型 Redis 集群不太适用，动态可扩展能力也有待证明。后经过反复深入讨论和技术预研，最终选定了 Codis 这款国人定制的集群方案，其采用 Proxy 机制，能够集中运维和管理，同时还支持在线扩容，非常适合较大规模的 Redis 集群。为了保证并发高可用性，采用了 Proxy 和 Redis 实例一比一的集群方案，本来担心 Proxy 与应用节点间桥接连接数过多，但研究代码并实际监控后发现其 Proxy 和 Redis 实例间只建立了很少的几个长连接，简直太理想了。现在乐视商城已经部署了多套大规模 Codis 集群，并已经构建出自己的 Go 代码分支，未来会分享到 Git 上。

<img src="http://ipad-cms.csdn.net/cms/attachment/201608/579ef90d0de20.jpg" alt="图2  防刷单系统设计" title="图2  防刷单系统设计" />

图2  防刷单系统设计

### 高并发下的 API 访问策略
抢购秒杀不仅对自身的程序逻辑和存储提出较高的要求，同时对底层依赖的 API 也有极高的可用性要求，为了实现高并发下底层商品、促销、活动等 API 的可靠服务，进行了三个层面的改造，分别就冷热数据分离、访问熔断、本地化缓存进行了系统重构，从而实现抢购、秒杀、购物车等高并发功能在大促时持续稳定运行。在业务优化期间，发现底层 API 提供的数据中绝大部分都是以相对静态的数据为主，修改或变化的频率非常低，只有少数几个关键属性需要即时性要求非常高；如果按照高即时性指标优化系统，将付出极高的代价，几乎难以接受，只有实现 API 层面的冷热数据分离，把即时性要求高的业务要求封装为新的 API，对于老的 API 按照静态化原则优化缓存机制。另外，在极端情况下底层系统性能波动是否触发瞬间整站不可用的多米诺效应也是我们关注的重点，为了避免这种情况，引入了访问熔断机制，当单个线程访问底层 API 超时或超过重试次数后，将不再等待，直接返回错误信息，避免前台应用挂死。那么效果如何呢？在大规模压测时，发现带有熔断机制的 API 响应时间曲线非常稳定，平均 TPS 性能有明显提高，在极限压力下应用没有出现假死现象。相反，没有熔断机制的 API 性能波动较大，极限压力下应用节点全部出现假死现象。对于缓存的应用，发现冷数据存储在大规模集中式缓存服务器上的成本很高，性能提升是非线性的，同时还要考虑高可用性等等问题。相比较而言，采用本地化缓存开发和维护成本很低，效费比非常高，单节点故障不会产生额外影响，重启一下应用就可以重新恢复，充分发挥分布式无状态、动态可扩容架构方面的优势。
## 深入理解 Apache Flink 核心技术

文/李呈祥

>Apache Flink （下简称 Flink）项目是大数据处理领域最近冉冉升起的一颗新星，其不同于其他大数据项目的诸多特性吸引了越来越多人的关注。本文将深入分析 Flink 的一些关键技术与特性，希望能够帮助读者对 Flink 有更加深入的了解，对其他大数据系统开发者也能有所裨益。本文假设读者已对 MapReduce、Spark 及 Storm 等大数据处理框架有所了解，同时熟悉流处理与批处理的基本概念。

### Flink 简介

Flink 核心是一个流式的数据流执行引擎，其针对数据流的分布式计算提供了数据分布、数据通信以及容错机制等功能。基于流执行引擎，Flink 提供了诸多更高抽象层的 API 以便用户编写分布式任务：

1. DataSet API， 对静态数据进行批处理操作，将静态数据抽象成分布式的数据集，用户可以方便地使用 Flink 提供的各种操作符对分布式数据集进行处理，支持 Java、Scala 和 Python。

2. DataStream API，对数据流进行流处理操作，将流式的数据抽象成分布式的数据流，用户可以方便地对分布式数据流进行各种操作，支持 Java 和 Scala。

3. Table API，对结构化数据进行查询操作，将结构化数据抽象成关系表，并通过类 SQL 的 DSL 对关系表进行各种查询操作，支持 Java 和 Scala。

此外，Flink 还针对特定的应用领域提供了领域库，例如：

Flink ML，Flink 的机器学习库，提供了机器学习 Pipelines API 并实现了多种机器学习算法。

Gelly，Flink 的图计算库，提供了图计算的相关 API 及多种图计算算法实现。

Flink 的技术栈如图1所示：

<img src="http://ipad-cms.csdn.net/cms/attachment/201602/56a980118a310.jpg" alt="图1 Flink技术栈" title="图1 Flink技术栈" />

图1 Flink 技术栈

此外，Flink 也可以方便地和 Hadoop 生态圈中其他项目集成，例如 Flink 可以读取存储在 HDFS 或 HBase 中的静态数据，以 Kafka 作为流式的数据源，直接重用 MapReduce 或 Storm 代码，或是通过 YARN 申请集群资源等。

### 统一的批处理与流处理系统

在大数据处理领域，批处理任务与流处理任务一般被认为是两种不同的任务，一个大数据项目一般会被设计为只能处理其中一种任务，例如 Apache Storm、Apache Smaza 只支持流处理任务，而 Aapche MapReduce、Apache Tez、Apache Spark 只支持批处理任务。Spark Streaming 是 Apache Spark 之上支持流处理任务的子系统，看似一个特例，实则不然——Spark Streaming 采用了一种 micro-batch 的架构，即把输入的数据流切分成细粒度的 batch，并为每一个 batch 数据提交一个批处理的 Spark 任务，所以 Spark Streaming 本质上还是基于 Spark 批处理系统对流式数据进行处理，和 Apache Storm、Apache Smaza 等完全流式的数据处理方式完全不同。通过其灵活的执行引擎，Flink 能够同时支持批处理任务与流处理任务。

在执行引擎这一层，流处理系统与批处理系统最大不同在于节点间的数据传输方式。对于一个流处理系统，其节点间数据传输的标准模型是：当一条数据被处理完成后，序列化到缓存中，然后立刻通过网络传输到下一个节点，由下一个节点继续处理。而对于一个批处理系统，其节点间数据传输的标准模型是：当一条数据被处理完成后，序列化到缓存中，并不会立刻通过网络传输到下一个节点，当缓存写满，就持久化到本地硬盘上，当所有数据都被处理完成后，才开始将处理后的数据通过网络传输到下一个节点。这两种数据传输模式是两个极端，对应的是流处理系统对低延迟的要求和批处理系统对高吞吐量的要求。Flink 的执行引擎采用了一种十分灵活的方式，同时支持了这两种数据传输模型。Flink 以固定的缓存块为单位进行网络数据传输，用户可以通过缓存块超时值指定缓存块的传输时机。如果缓存块的超时值为0，则 Flink 的数据传输方式类似上文所提到流处理系统的标准模型，此时系统可以获得最低的处理延迟。如果缓存块的超时值为无限大，则 Flink 的数据传输方式类似上文所提到批处理系统的标准模型，此时系统可以获得最高的吞吐量。同时缓存块的超时值也可以设置为0到无限大之间的任意值。缓存块的超时阈值越小，则 Flink 流处理执行引擎的数据处理延迟越低，但吞吐量也会降低，反之亦然。通过调整缓存块的超时阈值，用户可根据需求灵活地权衡系统延迟和吞吐量。

<img src="http://ipad-cms.csdn.net/cms/attachment/201602/56a9801abf0f6.jpg" alt="图2 Flink执行引擎数据传输模式" title="图2 Flink执行引擎数据传输模式" />

图2 Flink 执行引擎数据传输模式

在统一的流式执行引擎基础上，Flink 同时支持了流计算和批处理，并对性能（延迟、吞吐量等）有所保障。相对于其他原生的流处理与批处理系统，并没有因为统一执行引擎而受到影响从而大幅度减轻了用户安装、部署、监控、维护等成本。

### Flink 流处理的容错机制

对于一个分布式系统来说，单个进程或是节点崩溃导致整个 Job 失败是经常发生的事情，在异常发生时不会丢失用户数据并能自动恢复才是分布式系统必须支持的特性之一。本节主要介绍 Flink 流处理系统任务级别的容错机制。

批处理系统比较容易实现容错机制，由于文件可以重复访问，当某个任务失败后，重启该任务即可。但是到了流处理系统，由于数据源是无限的数据流，从而导致一个流处理任务执行几个月的情况，将所有数据缓存或是持久化，留待以后重复访问基本上是不可行的。Flink 基于分布式快照与可部分重发的数据源实现了容错。用户可自定义对整个 Job 进行快照的时间间隔，当任务失败时，Flink 会将整个 Job 恢复到最近一次快照，并从数据源重发快照之后的数据。Flink 的分布式快照实现借鉴了 Chandy 和 Lamport 在1985年发表的一篇关于分布式快照的论文，其实现的主要思想如下：

按照用户自定义的分布式快照间隔时间，Flink 会定时在所有数据源中插入一种特殊的快照标记消息，这些快照标记消息和其他消息一样在 DAG 中流动，但是不会被用户定义的业务逻辑所处理，每一个快照标记消息都将其所在的数据流分成两部分：本次快照数据和下次快照数据。

<img src="http://ipad-cms.csdn.net/cms/attachment/201602/56a98027ce574.jpg" alt="图3 Flink包含快照标记消息的消息流" title="图3 Flink包含快照标记消息的消息流" />

图3 Flink 包含快照标记消息的消息流

快照标记消息沿着 DAG 流经各个操作符，当操作符处理到快照标记消息时，会对自己的状态进行快照，并存储起来。当一个操作符有多个输入的时候，Flink 会将先抵达的快照标记消息及其之后的消息缓存起来，当所有的输入中对应该次快照的快照标记消息全部抵达后，操作符对自己的状态快照并存储，之后处理所有快照标记消息之后的已缓存消息。操作符对自己的状态快照并存储可以是异步与增量的操作，并不需要阻塞消息的处理。分布式快照的流程如图4所示：

<img src="http://ipad-cms.csdn.net/cms/attachment/201602/56a980328ee7a.jpg" alt="图4 Flink分布式快照流程图" title="图4 Flink分布式快照流程图" />

图4 Flink 分布式快照流程图

当所有的 Data Sink （终点操作符）都收到快照标记信息并对自己的状态快照和存储后，整个分布式快照就完成了，同时通知数据源释放该快照标记消息之前的所有消息。若之后发生节点崩溃等异常情况时，只需要恢复之前存储的分布式快照状态，并从数据源重发该快照以后的消息就可以了。

Exactly-Once 是流处理系统需要支持的一个非常重要的特性，它保证每一条消息只被流处理系统处理一次，许多流处理任务的业务逻辑都依赖于 Exactly-Once 特性。相对于 At-Least-Once 或是 At-Most-Once, Exactly-Once 特性对流处理系统的要求更为严格，实现也更加困难。Flink 基于分布式快照实现了 Exactly-Once 特性。

相对于其他流处理系统的容错方案，Flink 基于分布式快照的方案在功能和性能方面都具有很多优点，包括：

- 低延迟。由于操作符状态的存储可以异步，所以进行快照的过程基本上不会阻塞消息的处理，因此不会对消息延迟产生负面影响。

- 高吞吐量。当操作符状态较少时，对吞吐量基本没有影响。当操作符状态较多时，相对于其他的容错机制，分布式快照的时间间隔是用户自定义的，所以用户可以权衡错误恢复时间和吞吐量要求来调整分布式快照的时间间隔。

- 与业务逻辑的隔离。Flink 的分布式快照机制与用户的业务逻辑是完全隔离的，用户的业务逻辑不会依赖或是对分布式快照产生任何影响。

- 错误恢复代价。分布式快照的时间间隔越短，错误恢复的时间越少，与吞吐量负相关。

### Flink 流处理的时间窗口

对于流处理系统来说，流入的消息不存在上限，所以对于聚合或是连接等操作，流处理系统需要对流入的消息进行分段，然后基于每一段数据进行聚合或是连接。消息的分段即称为窗口，流处理系统支持的窗口有很多类型，最常见的就是时间窗口，基于时间间隔对消息进行分段处理。本节主要介绍 Flink 流处理系统支持的各种时间窗口。

对于目前大部分流处理系统来说，时间窗口一般是根据 Task 所在节点的本地时钟进行切分，这种方式实现起来比较容易，不会产生阻塞。但是可能无法满足某些应用需求，比如：

- 消息本身带有时间戳，用户希望按照消息本身的时间特性进行分段处理。

- 由于不同节点的时钟可能不同，以及消息在流经各个节点的延迟不同，在某个节点属于同一个时间窗口处理的消息，流到下一个节点时可能被切分到不同的时间窗口中，从而产生不符合预期的结果。

Flink 支持3种类型的时间窗口，分别适用于用户对于时间窗口不同类型的要求：

- perator Time。根据 Task 所在节点的本地时钟来切分的时间窗口。

- Event Time。消息自带时间戳，根据消息的时间戳进行处理，确保时间戳在同一个时间窗口的所有消息一定会被正确处理。由于消息可能乱序流入 Task，所以 Task 需要缓存当前时间窗口消息处理的状态，直到确认属于该时间窗口的所有消息都被处理，才可以释放，如果乱序的消息延迟很高会影响分布式系统的吞吐量和延迟。

- Ingress Time。有时消息本身并不带有时间戳信息，但用户依然希望按照消息而不是节点时钟划分时间窗口，例如避免上面提到的第二个问题，此时可以在消息源流入 Flink 流处理系统时自动生成增量的时间戳赋予消息，之后处理的流程与 Event Time 相同。Ingress Time可以看成是 Event Time 的一个特例，由于其在消息源处时间戳一定是有序的，所以在流处理系统中，相对于 Event Time，其乱序的消息延迟不会很高，因此对 Flink 分布式系统的吞吐量和延迟的影响也会更小。

### Event Time 时间窗口的实现

Flink 借鉴了 Google 的 MillWheel 项目，通过 WaterMark 来支持基于 Event Time 的时间窗口。

当操作符通过基于 Event Time 的时间窗口来处理数据时，它必须在确定所有属于该时间窗口的消息全部流入此操作符后才能开始数据处理。但是由于消息可能是乱序的，所以操作符无法直接确认何时所有属于该时间窗口的消息全部流入此操作符。WaterMark 包含一个时间戳，Flink 使用 WaterMark 标记所有小于该时间戳的消息都已流入，Flink 的数据源在确认所有小于某个时间戳的消息都已输出到 Flink 流处理系统后，会生成一个包含该时间戳的 WaterMark，插入到消息流中输出到 Flink 流处理系统中，Flink 操作符按照时间窗口缓存所有流入的消息，当操作符处理到 WaterMark 时，它对所有小于该 WaterMark 时间戳的时间窗口数据进行处理并发送到下一个操作符节点，然后也将 WaterMark 发送到下一个操作符节点。

为了保证能够处理所有属于某个时间窗口的消息，操作符必须等到大于这个时间窗口的 WaterMark 之后才能开始对该时间窗口的消息进行处理，相对于基于 Operator Time 的时间窗口，Flink 需要占用更多内存，且会直接影响消息处理的延迟时间。对此，一个可能的优化措施是，对于聚合类的操作符，可以提前对部分消息进行聚合操作，当有属于该时间窗口的新消息流入时，基于之前的部分聚合结果继续计算，这样的话，只需缓存中间计算结果即可，无需缓存该时间窗口的所有消息。

对于基于 Event Time 时间窗口的操作符来说，流入 WaterMark 的时间戳与当前节点的时钟一致是最简单理想的状况，但是在实际环境中是不可能的，由于消息的乱序以及前面节点处理效率的不同，总是会有某些消息流入时间大于其本身的时间戳，真实 WaterMark 时间戳与理想情况下 WaterMark 时间戳的差别称为 Time Skew，如图5所示：

<img src="http://ipad-cms.csdn.net/cms/attachment/201602/56a9803be1ca1.jpg" alt="图5 WaterMark的Time Skew图" title="图5 WaterMark的Time Skew图" />	

图5 WaterMark 的 Time Skew 图

Time Skew 决定了该 WaterMark 与上一个 WaterMark 之间的时间窗口所有数据需要缓存的时间，Time Skew 时间越长，该时间窗口数据的延迟越长，占用内存的时间也越长，同时会对流处理系统的吞吐量产生负面影响。

### 基于时间戳的排序

在流处理系统中，由于流入的消息是无限的，所以对消息进行排序基本上被认为是不可行的。但是在 Flink 流处理系统中，基于 WaterMark，Flink 实现了基于时间戳的全局排序。排序的实现思路如下：排序操作符缓存所有流入的消息，当其接收到 WaterMark 时，对时间戳小于该 WaterMark 的消息进行排序，并发送到下一个节点，在此排序操作符中释放所有时间戳小于该 WaterMark 的消息，继续缓存流入的消息，等待下一个 WaterMark 触发下一次排序。

由于 WaterMark 保证了在其之后不会出现时间戳比它小的消息，所以可以保证排序的正确性。需要注意的是，如果排序操作符有多个节点，只能保证每个节点的流出消息是有序的，节点之间的消息不能保证有序，要实现全局有序，则只能有一个排序操作符节点。

通过支持基于 Event Time 的消息处理，Flink 扩展了其流处理系统的应用范围，使得更多的流处理任务可以通过 Flink 来执行。

### 定制的内存管理

Flink 项目基于 Java 及 Scala 等 JVM 语言，JVM 本身作为一个各种类型应用的执行平台，其对 Java 对象的管理也是基于通用的处理策略，其垃圾回收器通过估算 Java 对象的生命周期对 Java 对象进行有效率的管理。

针对不同类型的应用，用户可能需要针对该类型应用的特点，配置针对性的 JVM 参数更有效率的管理 Java 对象，从而提高性能。这种JVM调优的黑魔法需要用户对应用本身及 JVM 的各参数有深入了解，极大地提高了分布式计算平台的调优门槛。Flink 框架本身了解计算逻辑每个步骤的数据传输，相比于JVM垃圾回收器，其了解更多的 Java 对象生命周期，从而为更有效率地管理 Java 对象提供了可能。

### JVM 存在的问题

#### Java 对象开销

相对于 c/c++ 等更加接近底层的语言，Java 对象的存储密度相对偏低，例如，“abcd”这样简单的字符串在 UTF-8 编码中需要4个字节存储，但采用了 UTF-16 编码存储字符串的 Java 则需要8个字节，同时 Java 对象还有 header 等其他额外信息，一个4字节字符串对象在 Java 中需要48字节的空间来存储。对于大部分的大数据应用，内存都是稀缺资源，更有效率地内存存储，意味着 CPU 数据访问吞吐量更高，以及更少磁盘落地的存在。

#### 对象存储结构引发的 cache miss

为了缓解 CPU 处理速度与内存访问速度的差距，现代 CPU 数据访问一般都会有多级缓存。当从内存加载数据到缓存时，一般是以 cache line 为单位加载数据，所以当 CPU 访问的数据如果是在内存中连续存储的话，访问的效率会非常高。如果 CPU 要访问的数据不在当前缓存所有的 cache line 中，则需要从内存中加载对应的数据，这被称为一次 cache miss。当 cache miss 非常高的时候，CPU 大部分的时间都在等待数据加载，而不是真正的处理数据。Java 对象并不是连续的存储在内存上，同时很多的 Java 数据结构的数据聚集性也不好。

#### 大数据的垃圾回收

Java 的垃圾回收机制一直让 Java 开发者又爱又恨，一方面它免去了开发者自己回收资源的步骤，提高了开发效率，减少了内存泄漏的可能，另一方面垃圾回收也是 Java 应用的不定时炸弹，有时秒级甚至是分钟级的垃圾回收极大影响了 Java 应用的性能和可用性。在时下数据中心，大容量内存得到了广泛的应用，甚至出现了单台机器配置 TB 内存的情况，同时，大数据分析通常会遍历整个源数据集，对数据进行转换、清洗、处理等步骤。在这个过程中，会产生海量的 Java 对象，JVM 的垃圾回收执行效率对性能有很大影响。通过 JVM 参数调优提高垃圾回收效率需要用户对应用和分布式计算框架以及 JVM 的各参数有深入了解，而且有时候这也远远不够。

#### OOM 问题

OutOfMemoryError 是分布式计算框架经常会遇到的问题，当 JVM 中所有对象大小超过分配给 JVM 的内存大小时，就会出现 OutOfMemoryError 错误，JVM 崩溃，分布式框架的健壮性和性能都会受到影响。通过 JVM 管理内存，同时试图解决 OOM 问题的应用，通常都需要检查 Java 对象的大小，并在某些存储 Java 对象特别多的数据结构中设置阈值进行控制。但是 JVM 并没有提供官方检查 Java 对象大小的工具，第三方的工具类库可能无法准确通用地确定 Java 对象大小。侵入式的阈值检查也会为分布式计算框架的实现增加很多额外与业务逻辑无关的代码。

### Flink 的处理策略

为了解决以上提到的问题，高性能分布式计算框架通常需要以下技术：

- 定制的序列化工具。显式内存管理的前提步骤就是序列化，将 Java 对象序列化成二进制数据存储在内存上（on heap 或是 off-heap）。通用的序列化框架，如 Java 默认使用 java.io.Serializable 将 Java 对象及其成员变量的所有元信息作为其序列化数据的一部分，序列化后的数据包含了所有反序列化所需的信息。这在某些场景中十分必要，但是对于 Flink 这样的分布式计算框架来说，这些元数据信息可能是冗余数据。定制的序列化框架，如 Hadoop 的 org.apache.hadoop.io.Writable 需要用户实现该接口，并自定义类的序列化和反序列化方法。这种方式效率最高，但需要用户额外的工作，不够友好。

- 显式的内存管理。一般通用的做法是批量申请和释放内存，每个JVM实例有一个统一的内存管理器，所有内存的申请和释放都通过该内存管理器进行。这可以避免常见的内存碎片问题，同时由于数据以二进制的方式存储，可以大大减轻垃圾回收压力。

- 缓存友好的数据结构和算法。对于计算密集的数据结构和算法，直接操作序列化后的二进制数据，而不是将对象反序列化后再进行操作。同时，只将操作相关的数据连续存储，可以最大化的利用 L1/L2/L3 缓存，减少 Cache miss 的概率，提升 CPU 计算的吞吐量。以排序为例，由于排序的主要操作是对 Key 进行对比，如果将所有排序数据的 Key 与 Value 分开并对 Key 连续存储，那么访问 Key 时的 Cache 命中率会大大提高。

#### 定制的序列化工具

分布式计算框架可以使用定制序列化工具的前提是要待处理数据流通常是同一类型，由于数据集对象的类型固定，从而可以只保存一份对象 Schema 信息，节省大量的存储空间。同时，对于固定大小的类型，也可通过固定的偏移位置存取。在需要访问某个对象成员变量时，通过定制的序列化工具，并不需要反序列化整个 Java 对象，而是直接通过偏移量，从而只需要反序列化特定的对象成员变量。如果对象的成员变量较多时，能够大大减少 Java 对象的创建开销，以及内存数据的拷贝大小。Flink 数据集都支持任意 Java 或是 Scala 类型，通过自动生成定制序列化工具，既保证了 API 接口对用户友好（不用像 Hadoop 那样数据类型需要继承实现 org.apache.hadoop.io.Writable 接口），也达到了和 Hadoop 类似的序列化效率。

Flink 对数据集的类型信息进行分析，然后自动生成定制的序列化工具类。Flink 支持任意的 Java 或是 Scala 类型，通过 Java Reflection 框架分析基于 Java 的 Flink 程序 UDF（User Define Function）的返回类型的类型信息，通过 Scala Compiler分析基于 Scala 的 Flink 程序 UDF 的返回类型的类型信息。类型信息由 TypeInformation 类表示，这个类有诸多具体实现类，例如：

- BasicTypeInfo 任意 Java 基本类型（装包或未装包）和 String 类型。

- BasicArrayTypeInfo 任意 Java 基本类型数组（装包或未装包）和 String 数组。

- WritableTypeInfo 任意 Hadoop 的 Writable 接口的实现类。

- TupleTypeInfo 任意的 Flink tuple 类型(支持 Tuple1 to Tuple25). Flink tuples 是固定长度固定类型的 Java Tuple 实现。

- CaseClassTypeInfo 任意的 Scala CaseClass (包括 Scala tuples)。

- PojoTypeInfo 任意的 POJO (Java or Scala)，例如 Java 对象的所有成员变量，要么是 public 修饰符定义，要么有getter/setter 方法。

- GenericTypeInfo 任意无法匹配之前几种类型的类。

前6种类型数据集几乎覆盖了绝大部分的 Flink 程序，针对前6种类型数据集，Flink 皆可以自动生成对应的 TypeSerializer 定制序列化工具，非常有效率地对数据集进行序列化和反序列化。对于第7种类型，Flink 使用 Kryo 进行序列化和反序列化。此外，对于可被用作 Key 的类型，Flink 还同时自动生成 TypeComparator，用来辅助直接对序列化后的二进制数据直接进行 compare、hash 等操作。对于 Tuple、CaseClass、Pojo 等组合类型，Flink 自动生成的 TypeSerializer、TypeComparator 同样是组合的，并把其成员的序列化/反序列化代理给其成员对应的 TypeSerializer、TypeComparator，如图6所示：

<img src="http://ipad-cms.csdn.net/cms/attachment/201602/56a9804eb1702.jpg" alt="图6 Flink组合类型序列化" title="图6 Flink组合类型序列化" />

图6 Flink 组合类型序列化

此外如有需要，用户可通过集成 TypeInformation 接口定制实现自己的序列化工具。

#### 显式的内存管理

垃圾回收是 JVM 内存管理回避不了的问题，JDK8 的 G1 算法改善了 JVM 垃圾回收的效率和可用范围，但对于大数据处理实际环境还远远不够。这也和现在分布式框架的发展趋势有所冲突，越来越多的分布式计算框架希望尽可能多地将待处理数据集放入内存，而对于 JVM 垃圾回收来说，内存中 Java 对象越少、存活时间越短，其效率越高。通过 JVM 进行内存管理的话，OutOfMemoryError 也是一个很难解决的问题。同时，在 JVM 内存管理中，Java 对象有潜在的碎片化存储问题（Java 对象所有信息可能在内存中连续存储），也有可能在所有 Java 对象大小没有超过 JVM 分配内存时，出现 OutOfMemoryError 问题。Flink 将内存分为3个部分，每个部分都有不同用途：

- Network buffers: 一些以32 KB Byte 数组为单位的 buffer，主要被网络模块用于数据的网络传输。

- Memory Manager pool 大量以32KB Byte 数组为单位的内存池，所有的运行时算法（例如Sort/Shuffle/Join）都从这个内存池申请内存，并将序列化后的数据存储其中，结束后释放回内存池。

- Remaining (Free) Heap 主要留给 UDF 中用户自己创建的 Java 对象，由 JVM 管理。

Network buffers 在 Flink 中主要基于 Netty 的网络传输，无需多讲。Remaining Heap 用于 UDF 中用户自己创建的 Java 对象，在 UDF 中，用户通常是流式的处理数据，并不需要很多内存，同时 Flink 也不鼓励用户在UDF中缓存很多数据，因为这会引起前面提到的诸多问题。Memory Manager pool （以后以内存池代指）通常会配置为最大的一块内存，接下来会详细介绍。

在 Flink 中，内存池由多个 MemorySegment 组成，每个MemorySegment 代表一块连续的内存，底层存储是byte[]，默认32KB大小。MemorySegment 提供了根据偏移量访问数据的各种方法，如 get/put int、long、float、double 等，MemorySegment 之间数据拷贝等方法和 java.nio.ByteBuffer 类似。对于 Flink 的数据结构，通常包括多个向内存池申请的 MemeorySegment，所有要存入的对象通过 TypeSerializer 序列化之后，将二进制数据存储在 MemorySegment 中，在取出时通过 TypeSerializer 反序列化。数据结构通过 MemorySegment 提供的 set/get 方法访问具体的二进制数据。Flink 这种看起来比较复杂的内存管理方式带来的好处主要有：

- 二进制的数据存储大大提高了数据存储密度，节省了存储空间。

- 所有的运行时数据结构和算法只能通过内存池申请内存，保证了其使用的内存大小是固定的，不会因为运行时数据结构和算法而发生 OOM。对于大部分的分布式计算框架来说，这部分由于要缓存大量数据最有可能导致 OOM。

- 内存池虽然占据了大部分内存，但其中的 MemorySegment 容量较大（默认32KB），所以内存池中的 Java 对象其实很少，而且一直被内存池引用，所有在垃圾回收时很快进入持久代，大大减轻了 JVM 垃圾回收的压力。

- Remaining Heap 的内存虽然由 JVM 管理，但是由于其主要用来存储用户处理的流式数据，生命周期非常短，速度很快的 Minor GC 就会全部回收掉，一般不会触发 Full GC。

Flink 当前的内存管理在最底层是基于 byte[]，所以数据最终还是 on-heap，最近 Flink 增加了 off-heap 的内存管理支持。Flink off-heap 的内存管理相对于 on-heap 的优点主要在于：

- 启动分配了大内存(例如100 G)的 JVM 很耗费时间，垃圾回收也很慢。如果采用 off-heap，剩下的 Network buffer 和 Remaining heap 都会很小，垃圾回收也不用考虑 MemorySegment 中的 Java 对象了。

- 更有效率的 IO 操作。在 off-heap 下，将 MemorySegment 写到磁盘或是网络可以支持 zeor-copy 技术，而 on-heap 的话则至少需要一次内存拷贝。

- off-heap 可用于错误恢复，比如 JVM 崩溃，在 on-heap 时数据也随之丢失，但在 off-heap 下，off-heap 的数据可能还在。此外，off-heap 上的数据还可以和其他程序共享。

#### 缓存友好的计算

磁盘 IO 和网络 IO 之前一直被认为是 Hadoop 系统的瓶颈，但是随着 Spark、Flink 等新一代分布式计算框架的发展，越来越多的趋势使得 CPU/Memory 逐渐成为瓶颈，这些趋势包括：

- 更先进的 IO 硬件逐渐普及。10GB 网络和 SSD 硬盘等已经被越来越多的数据中心使用。

- 更高效的存储格式。Parquet，ORC 等列式存储被越来越多的 Hadoop 项目支持，其非常高效的压缩性能大大减少了落地存储的数据量。

- 更高效的执行计划。例如很多 SQL 系统执行计划优化器的 Fliter-Push-Down 优化会将过滤条件尽可能的提前，甚至提前到 Parquet 的数据访问层，使得在很多实际的工作负载中并不需要很多的磁盘 IO。

由于 CPU 处理速度和内存访问速度的差距，提升 CPU 的处理效率的关键在于最大化的利用 L1/L2/L3/Memory，减少任何不必要的 Cache miss。定制的序列化工具给 Flink 提供了可能，通过定制的序列化工具，Flink 访问的二进制数据本身，因为占用内存较小，存储密度比较大，而且还可以在设计数据结构和算法时尽量连续存储，减少内存碎片化对 Cache 命中率的影响，甚至更进一步，Flink 可以只是将需要操作的部分数据（如排序时的 Key）连续存储，而将其他部分的数据存储在其他地方，从而最大可能地提升 Cache 命中的概率。

以 Flink 中的排序为例，排序通常是分布式计算框架中一个非常重的操作，Flink 通过特殊设计的排序算法获得了非常好的性能，其排序算法的实现如下：

- 将待排序的数据经过序列化后存储在两个不同的 MemorySegment 集中。数据全部的序列化值存放于其中一个 MemorySegment 集中。数据序列化后的 Key 和指向第一个 MemorySegment 集中值的指针存放于第二个 MemorySegment 集中。

- 对第二个 MemorySegment 集中的 Key 进行排序，如需交换 Key 位置，只需交换对应的Key+Pointer的位置，第一个MemorySegment集中的数据无需改变。 当比较两个Key大小时，TypeComparator提供了直接基于二进制数据的对比方法，无需反序列化任何数据。

- 排序完成后，访问数据时，按照第二个 MemorySegment 集中 Key 的顺序访问，并通过 Pointer 值找到数据在第一个 MemorySegment 集中的位置，通过 TypeSerializer 反序列化成 Java 对象返回。

<img src="http://ipad-cms.csdn.net/cms/attachment/201602/56a98059aae38.jpg" alt="图7 Flink排序算法" title="图7 Flink排序算法" />

图7 Flink 排序算法

这样实现的好处有：

- 通过 Key 和 Full data 分离存储的方式尽量将被操作的数据最小化，提高 Cache 命中的概率，从而提高 CPU 的吞吐量。

- 移动数据时，只需移动 Key+Pointer，而无须移动数据本身，大大减少了内存拷贝的数据量。

- TypeComparator 直接基于二进制数据进行操作，节省了反序列化的时间。

通过定制的内存管理，Flink 通过充分利用内存与 CPU 缓存，大大提高了 CPU 的执行效率，同时由于大部分内存都由框架自己控制，也很大程度提升了系统的健壮性，减少了 OOM 出现的可能。

### 总结

本文主要介绍了 Flink 项目的一些关键特性，Flink 是一个拥有诸多特色的项目，包括其统一的批处理和流处理执行引擎，通用大数据计算框架与传统数据库系统的技术结合，以及流处理系统的诸多技术创新等，因为篇幅有限，Flink 还有一些其他很有意思的特性没有详细介绍，比如 DataSet API 级别的执行计划优化器，原生的迭代操作符等，感兴趣的读者可以通过 Flink 官网了解更多 Flink 的详细内容。希望通过本文的介绍能够让读者对 Flink 有更多的了解，也让更多的人使用甚至参与到 Flink 项目中去。
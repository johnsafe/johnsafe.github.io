## 阅文集团分布式文件系统的设计与实现

文/徐海峰

阅文是腾讯与盛大文学整合成立的集团，它们设计并部署了一套分布式文件系统（DFS），运行在廉价的 Linux 系统中，具有良好的可伸缩性和海量数据访问能力， 已为其书库提供存储与访问能力，本文主要讨论这套系统的架构和特性。

### 简介
DFS 系统目前的业务主要用于存储文章的章节内容。对于一个阅读性质的网站，其中最大的存储量就是由作家生成的文章，这些文章量大并不适合使用传统的关系型数据库存储。将文章在系统中抽象，它仅仅是一个对象，所以我们首先想到了将它存储在某个固定的文件系统中。但是目前很少有文件系统能保证水平扩展的能力，可以增加机器来解决数据量和访问量的问题。对于数据来说，它还应该具备维护数据的完整性、一致性的能力。也许开源中最合适的就是 FastDFS 了，但是它同组内完全镜像的方式让人无法接受， 不能实现增加机器就可以平滑提开性能和吞吐量。

首先，DFS 不需要保存信息的元数据。在整个系统中，文章内容只是文章的一个属性，而并非其全部信息，所以在关系型数据库中保存文章的属性信息在所难免这种模式就决定了可以将分布式文件系统做成类似于 KV 的结构。也是因为没有元信息，整个系统架构将会大大简化。相比 Google 的 GFS 或者是 Apache 开源的 HBase，去掉了 metedata 服务器，整个系统唯一的单点隐患被剔除。可用性上也得到了很大提升，而相对于 DFS 系统本身，通过 Key 可以精确定位所需要的内容。

其次，和传统的 DFS 的一个很大不同是数据将会面临经常被更改。不仅仅只是尾部增加操作，而是不知修改位置的增删改兼具。又因为没有了元数据信息，所以存储内容必须自带元信息，而且必须给用户适当的放大或缩小功能，所以我们将文件稀疏化，合理地放置了文件空洞来解决频繁的增删改问题。

第三，放弃了系统级别的元信息导致内容自带自解析信息，所以数据的一致性也是一个需要解决的问题。没有了元信息服务器，就没有了可以唯一控制文件被线程安全写入的机制，产生无法控制同步中对于同一部分内容先后执行次序的问题， 这一部分将会在数据一致性模型中详细讨论。

### 设计概述
#### 目标
在设计满足所需要的 DFS 的过程中，目标既有已经被证明可行的，也有相当一部分需要自己摸索。可谓机会和挑战并存。整个系统必须可以易于水平扩展，尽量做到增加机器就可以完成系统的存储量和吞吐量提升，而且这部分工作必须让系统自动完成，随着机器增多。手动管理将无法从容应对。

1. 系统也必须支持一定界限之内的垂直扩展，类似增加磁盘挂载点、磁盘容量等操作也必须要在系统给予支持，并且这个支持完全是自动化的。系统必须由廉价机器组成，所以机器失效应该是经常发生的故障。在整个系统中，这个故障将被看作是正常事件，系统必须能快速（30s之内）发现这个异常，将访问避开故障机器。

2. 系统必须充分发挥每一台机器的性能，简单的主从模式并不是首选，我们更需要多主的模式来应对海量的访问。

3. 系统必须支持大小文件的存储和访问，业务存储数据量基本都是文章内容，一般在 KB 级别，但也会有一些完本的数据等需要存放。所以存储将变成 MB 级别，对于这两者必须都支持，对于 GB 级别的大文件，腾讯有 TFS 来使用。

系统必须保证数据的安全性和一致性。不需要太说明，几乎每个系统都需要保证这2点；

1. 系统必须支持数据频繁更改，对于 DFS 来说 modify 最难实现，不仅会引起文件空洞问题，更是数据一致性杀手。很多 DFS 不会从根本上支持 modify 功能，它们把 modify 认为是一次新的 insert，再通过版本号或类似方法来确定先后顺序，最后再通过合并减少文件空洞来解决掉数据一致性问题；但是业务决定了必须面对该问题，相对于把 modify 认为是 insert，磁盘浪费实在太大，得不偿失。

2. 增加机器、磁盘或挂载点，都不会让数据再平衡。在分布式系统中，数据再平衡往往是耗时最长的工作，且控制度也不是很准确，常需要多种措施一起才能保证数据的平衡和一致，所以在系统中，除了灾难恢复或同步以外，不存在任何的数据再平衡现象。
#### 架构
为达成目标，对 DFS 进行了最简化的设计，和目前大家常用的 DFS 不同，我们去掉了 DFS 元数据功能，但保留了该服务器节点（以下统称该节点为 Tracker）。此后服务器只剩下了 Tracker 和存储服务器（以下简称 Storage）。

<img src="http://ipad-cms.csdn.net/cms/attachment/201604/56fb912069134.jpg" alt="图1 DFS架构图" title="图1 DFS架构图" />

图1 DFS 架构图

Tracker 主要用途从存储元信息变成一个状态控制服务器和负载均衡服务器，或者说是一个类目录服务器。一方面它主要和系统中的 Storage 配合，对 Storage 进行健康检查和状态维护；另一方面，Tracker 还需要对客户端的访问进行路由工作。这样的无元数据设计不仅从根本上解决了元数据导致的单点问题，而且还减轻了数据的完整性在系统中的压力。从实现角度看，也大大减少了代码量，使 Tracker 节点变成一个可任意水平扩展的节点，从而解决了系统中最不稳定的一环。

相对 Tracker 来说，Storage 的工作将会复杂很多，它不仅要做文件系统本身的“增删改查”工作，还需要肩负起步工作。为了简单，把 Storage 的存储文件只做镜像同步，不做相对复杂的打散再存储工作。也为了尽量合理地利用机器并且可以快速通过增加机器的方式来达到对于存储量和性能的提升，也对同组内的 Storage 进行了分片处理，即逻辑集群。在同一个组内，分成一定量的 SyncGroup，每一个 SyncGroup 内部的服务器是镜像的，不同 SyncGroup 内的 Storage 之间是平行的，在同步上不存在任何关系。这一设计在维护的时候增加了一个人工的 SyncGroup 工作，但是它简化了同步打散的算法。最重要的是增加机器不需要移动原先的数据，使得增加机器好似在增加 DFS 的存储量和吞吐能力。

Storage 的改变不会迁移数据，也会导致性能的热点问题。新加的 Storage 负载可能无法支撑老数据的频繁访问，由此老的数据基本上都用来做读取之用，业务服务可以规避掉这个问题。对于写来说，本身不会有太多请求，所以这样的设计是我们所能接受的。

Tracker 和 Storage 在 Linux 中是一个用户层面的进程，并未涉及到系统内核，都有自己的 API，并没有为了兼容 POIXS 等接口带来复杂度。方便了以后的维护和管理，以及实现时只需考虑 DFS 本身问题即可。
#### 类目录服务
上面提到节省掉“元数据”信息，使得整个设计都变成了“无中心化”意味着在 DFS 中不会存在目录服务。但为了满足 DFS 容灾需求，设计了一种轻量级的类目录服务，服务不会具体提供到数据存储的目录或文件，而仅是提供存储服务器。又因为逻辑集群内的 Storage 之间的镜像特性，所以数据迁移问题虽然省略掉了元数据，但可以用这种类目录的方式定位到 Storage。

定位服务有点类似于 Ceph 的 Crush 算法，但没有 Ceph 灵活，而是固定了一些 Ceph 设置灵活的条件。在定位服务中，永远只会有一个满足条件的 Storage 被返回，以驱动数据访问。这得益于逻辑集群内的镜像设计，它不会把数据打散或重新平衡，只要逻辑集群内的 Storage 保证会完全镜像，所以返回一个 Storage 也足以应对需求。

在处理满足条件的 Storage 之中，也会根据实际的情况对 Storage 进行筛选。在 Tracker 中，所有 Storage 都被保存成一种 HashTree 结构，通过 GroupName 和 SyncGroup 来分组，然后通过心跳、磁盘大小等物理条件和设置的平衡策略来选择唯一满足的 Storage 来提供服务。这样设计简化了算法，也保证了系统事务性。
#### 存储模型
在整个系统中，文章只是一个对象，在 DFS 中，文章内容虽然也是对象，但它被分成了2种：即小文件和大文件。对于文件的区分，可通过设定一个阀值决定。

大文件将作为 SingleFile 单独存储。它不会带有别的信息，唯一使用的信息是 LastModifyTime，这部分在访问和同步中有涉及。

对于小文件，DFS 会把这些零散的内容合并成一个 chunkfile，在合并内容的同时，也把零散的随机 IO 变成了顺序 IO。这样不仅解决了海量小文件对于系统压力的问题，也解决了随机 IO 导致的性能低下的问题，一举两得。相对于 SingleFile，ChunkFile 的内容存储就不同。DFS 无法简单只存储内容，所以会给每一个需要存储的内容加一个 Metadata。Metedata 主要存储了文件的存储日期、是否被删除、最后更新时间/总长度/实际使用长度等一序列的数据元信息。但这些属性里面最重要的是一个类似 MVCC 的值，在 DFS 中是 opver（操作版本）解决了版本问题。

<img src="http://ipad-cms.csdn.net/cms/attachment/201604/56fb912c12a8d.jpg" alt="图2  元信息结构图" title="图2  元信息结构图" />

图2  元信息结构图

DFS 关心的是文件名字，不管是 SingleFile 还是 ChunkFile，DFS 只受限于名字不能产生冲突。所以文件名必须带上 Storage 的 MachineID。MachineID 在 Group 内部具有唯一性。文件名带上 MachineID 后，首先可确保文件归属问题，把唯一范围缩小到单机，而单机内部还不能保证文件的唯一性。所以，再给文件名带上另外的一些信息，包括但不限于创建时间、顺序数、线程 ID 号，至少由这4部分组成的文件名才能确保该文件在 DFS 系统内的唯一性。

对于 Storage，首选使用多而小的磁盘，可让系统并发达到最大化。而对于整个系统来说，又必须要支持磁盘大容量，所以增加了多挂载点功能。原则上，每台 Storage 机器最多可支持256个挂载点，这些挂载点会根据一定的规则进行数据平衡策略。通常采用轮询策略，也支持最大剩余空间等策略，这部分可以根据需要自己决定。对于多挂载点的支持，同一逻辑集群内必须做到镜像。

对于磁盘剩余空间，DFS 并不会使用全部磁盘空间来存储数据。具体最后剩余多大的空间会让系统的使用者根据实际使用情况来确定，一般推荐值是大于4G，该数字并非信口开河设置，而是根据 Storage 和 Tracker 时间的心跳时间差和单位时间内最大上传量计算所得。如果想把剩余空间缩小，就可能需要先把心跳时间差缩小（现在是30s），从而会导致过于频繁的心跳检查，对 Tracker 会造成一定的影响。
#### 线程安全
为达到多线程性能最大化，设计了一个多线程安全模型，即对象线程化。

在多线程环境中，最大性能消耗在于处理资源竞争锁，所以怎样避免锁出现成为了性能优劣的大问题。为了更好地优化性能，接着又加上对象池概念，把对象线程化范围只缩小到在该对象被访问使用期间。

对象线程化是每当有访问进来时，DFS 会根据访问的不同分发到不同的服务器。一般 insert 会被随机分配，但删改查操作就会被通过一个算法分配到一个 Task Module 中的线程处理，这样通过 Task Module 中的 Loop 进行操作排队，操作永远都被串行化。所以这些操作之间不会对同一块数据进行并发操作，以此来达到访问操作串行化。

当然，仅使用这个办法还无法彻底解决数据的线程安全和资源的竞争锁问题，还需要在写入磁盘时用磁盘文件进行配合。使用同样的方法，可将磁盘文件也进行线程化，对于 SingleFile，不需要任何线程化控制；对于 ChunkFile 文件，它的线程化处理方法就是每个线程对应一个文件，直到将这个文件写满，否则将一直写，从而解决了在数据落入磁盘时的竞争锁问题。但是它并不是完美的，停机时因为 ChunkFile 会被关闭，而它已经申请但是未被写入数据的部分大小将会形成文件空洞，且该文件空洞是无法避免和再次利用的，所以这部分损失将被系统直接忽略。

这里涉及了 ChunkFile 文件大小的问题。相对 DFS 来说，ChunkFile 大小并无多少影响，只需要从磁盘的使用率和系统调用的性能出发来合理地确定一个值。目前我们使用的值是64MB，数字来源于区分 SingleFile 和 ChunkFile 的阀值，把小于1MB 的数据划分入 ChunkFile，大于1MB 的数据被认为是大文件，使用 SingleFile 存储。而64MB 相当于最少存储64个文章章节，这种系统调用开销还算能接受。从目前的实际使用情况来看，当章节数据量远远小于1MB，一般就是 KB 级别，所以下一个版本中，我们的 ChunkFile 可能会成倍地缩小，应该会在4MB-8MB 之间。
#### 文件空洞问题
DFS和一些开源 DFS 最大的不同是对于 modify 的支持。一些开源 DFS 基本不会考虑 modify 支持，比如 LevelDB 等，是通过版本号然后合并的方式来规避掉对于 modify 的直接支持。这样对于这些 DFS 来说，就不会存在比较棘手的文件空洞问题。

文件空洞是由于 DFS 支持更改、删除这些操作，导致文件内容断断续续，不会形成一整块。虽然对正常使用影响不大，但是对于性能还是有一定影响的，文件空洞问题只会存在 ChunkFile 中，而 SingleFile 是单独文件，整个都会交给系统处理,所以 SingleFile 不需要考虑。在使用空间方面支持频繁 modify 功能的 DFS，会浪费不小空间，所以必须要解决这个问题。

解决这个问题方法分成2步走：

1. 对于每次 insert 操作，都预留了一部分的空间给 modify 支持。预留空间量可通过相对应的业务决定，目前支持绝对值或百分比增量。经过几次业务的观察，发现20%量最合适。对于现有 DFS 磁盘来说，是可以接受的。扩充20%之后，后面的每次 modify 只要量不超过这个值直接使用这块空间，就不需要再次申请空间。从性能角度来说比不上顺序写（此时是随机写），但这样写的方式依然可接受。

2. modify 操作增量超过阀值时，只能重新申请空间，但以前那一块空间也不能浪费，浪费就会出现文件空洞，所以会有一个管理这个废弃空间的 SkipList，当 insert 时，会优先使用这些废弃空间，来实现消除文件空洞和合理利用空间的效果。

对于顺序写和随机写，从性能角度来说，顺序写远好于随机写。但不管怎样，即使把所有随机写全部改成顺序写，也都会出现1bit 的随机读写（需要把 isdelete 置位）。所以性能开销并不大，如果使用顺序写+版本号，后续可使用合并的功能替代掉。
### 数据完整性
#### 日志
对于所有的完整性，日志都是一个关键时刻救命的一票。DFS 也不例外，数据完整性相关的日志分为2类：BinLog 和 SyncLog。

为什么没有常见的 redo？因为并不存在多数据刷入磁盘的时间间隔问题。完全是一个KV数据库，没有各种 RMDB 的繁琐操作，更没有事务的概念，DFS 直接对某一个值的操作，不存在恢复数据这一说。

1. BinLog 主要记录了对于 DFS 的增删改操作，根据不同的操作，记录的格式和内容不一样。

2. SyncLog 主要记录了对于 DFS 同步时，已经同步到的记录。SyncLog 的格式和 BinLog 一致，可以认为 SyncLog 是 BinLog 在 Remote Machine 上的一个备份文件。
#### 同步
DFS 的同步分为2部分组成。一部分是通常意义上的同步，主要目的是完成运行时同一 SyncGroup 内部的数据镜像问题；另外一部分因为 DFS 的特点，在设计时就认为 DFS 的机器是易损坏的且不可能完全通过本机磁盘恢复的，所以我们增加了一个灾难恢复功能，主要用来在磁盘和机器出现问题，或增加机器时，数据重新自动平衡自用。
#### 实时同步
对于实时同步，DFS 使用 Gossip 算法，通过遍历本机 BinLog，把数据采用 Push 的方式同步到 Remote Storage。这些 Remote Storage 必须拥有同样的 GroupName 和 SyncGroup。因为 DFS 总体上就是“去中心化”的设计，每台 Storage 在成为 master 的同时又都扮演 Slaver 角色，所以在同步时，并没有一个固定的参照来作为标准，这里使用了自创的“分布式同步状态机”来完成。

分布式同步状态机是各 Storage 都会维护一个同步状态，在 Storage 启动时重新启动同步时做状态机的校验。因为网络、同步延迟和同步方式的关系， 统一将 Remote Storage 上的状态作为当前同步的起始点，本地 Storage 获取 Remote Storage 上的状态后，才会构建同步状态机。

为了简单，同步只是发生在 Storage 之间，实际上并没有 Tracker 任何实际性的事情。Tracker 在整个同步中不会充当任何的角色，它仅为 Storage 提供了一个获取 Remote Storage 的功能，类似于提供了目录服务。Storage 会周期性地获取 Remote Storage 信息，然后通过获取的 Remote Storage 来构建同步。

同步数据会随着 BinLog 的日志一并发送给 Remote Storage，Remote Storage 会在处理完同步后，把日志记录到 SyncLog，以表明已经接受此信息的同步。SyncLog 不仅显式提供信息确认，它还在灾难恢复中充当极其重要的角色。

因为实时同步根据 BinLog 文件的增加来完成同步功能，可能会在文件属性更改的窗口期发生延迟现象。目前还没有找到更好的方式来解决这个办法，也许使用一个内存内的变量，但目前这个方案还仅仅在构思阶段，并未进入实际的系统。
#### 灾难恢复
灾难恢复对于 DFS 来说是很好的补充。从系统设计开始，目标就是把廉价的磁盘组合起来，形成可以存储大容量的设备。但这些廉价磁盘不可避免地发生损坏或者下线，所以磁盘损坏后的数据修复工作就变得极其重要。而且随着机器的增多，手工完成这部分工作显然是不可接受的意味着这部分必须自动化完成。这就是灾难恢复的由来，它给系统带来了完整性，解放了运维的痛苦。

灾难恢复和实时同步采取的算法大同小异，也是使用 Gossip 的方式。但灾难恢复并不采用 Push 方式，而是采用了相反的 Pull 方式来执行。究其原因：对于实时同步来说，如果是采用 Pull 方式，那么必须要经过2次的通讯才能完成一条记录的同步，而采用 Push 方式只需要一次合并后的通讯就可以解决问题，显然 Push 方式更合适。而且对于实时同步，Salver 机器并不知道需要同步的数据和时间点，所以由 Master 主动发起同步更合适。但是灾难恢复的发生条件是本地的磁盘损坏了，需要重新把这些数据从同一 SyncGroup 内的机器上同步过来。对于发生灾难恢复的机器来说，本地是存在以前的同步记录的，所以这时候知道需要同步的数据，而且不管是 Pull 还是 Push 方式，都需要同样的网络通讯次数（同样是1次）才能同步一条数据，所以这里使用 Pull 方式更合适。

但凡事都有例外，如果 DFS 的日志文件丢失怎么办？DFS 在执行灾难恢复之前会首先检查我们的 SyncLog 和 BinLog 在不在，如果不在会先同步 SyncLog 和 BinLog，把这些文件同步过来后，再通过这些文件恢复数据。唯一不好处理的是必须校验所有 SyncLog 和 BinLog，以免发生两者不一致的情况，导致数据同步不完整，引起系统数据不完整的情况。
#### 机器的改变和数据迁移
DFS 中机器数量改变不会发生数据迁移，这是从一开始就完全确定的设计方案。所以将DFS 设计成一个巨大的集群，但是在这个大集群内部会被分割成几个相互独立的，并无任何关系的逻辑集群。这些逻辑集群只是显式地处于一个大集群中而已，它们之间不会发生任何相互的状态、文件关系，数据的迁移只会发生在逻辑集群内部。

这些逻辑集群并不由系统随意决定，也非像 Ceph 那样的 Crush 算法来决定，而是机器被安放到大集群内部时就已经被标明所属的逻辑集群。也就是说其实这些逻辑集群内部的确定也确定了另一个重要的事情：一个文件会被备份的份数。因为逻辑集群内部是镜像的，所以这个备份分数就是逻辑集群的机器数-1，这个备份数也会因逻辑集群的人为指定而变成人为确定。

这样做的坏处是人为决定因素太多，但带来的好处是在机器数量发生变化，数据不会发生迁移（注意：这里只是说不会发生迁移，但是相互之间的备份还是会发生的）。这种数据量的备份已经最小化地节省了数据平衡的时间，也不会额外带来数据迁移压力，这是目前所能接受的最低底限了。
## Spark 在蘑菇街的实践

文/马永刚

蘑菇街每天有海量数据产生，由于社会化的因素，公司的业务非常复杂。海量的数据加上复杂的业务，使得数据分析变得很棘手。怎样处理和利用这些数据，给用户提供更好的服务，创造更大的社会价值，是困扰技术团队的一大难题。Spark 作为大数据领域最具人气的开源项目之一，以其高性能的计算能力、丰富的生态，赢得了大数据分析领域众多用户的青睐。蘑菇街基于 Spark 技术在搜索、排序、个性化推荐、广告、反作弊、用户画像等领域开展了许多积极的工作，取得了非常好的结果，本文结合 Spark 技术，分享 Spark 在蘑菇街的实践。

### 蘑菇街的 Spark 生态

蘑菇街对 Spark 的定位主要是：ETL+复杂计算。通过 Spark Streaming + Kafka 的方式获取数据，对数据进行清洗和简单的聚合之后存储到 HBase 和 Hive。HBase 的数据用于在线模型训练，Hive 的数据用于离线训练，基于这种架构，可以近实时收集站内各种业务数据，供后续的业务分析使用。Spark MLib 提供了很多基础的机器学习算法的封装，如回归、分类等，GraphX 提供了简单易操作的图计算模型，利用这些算法可以非常方便的完成机器学习的模型训练与评估，训练的模型可以快速用于线上排名、个性化推荐等业务。图1展示了蘑菇街的 Spark 生态。

<img src="http://ipad-cms.csdn.net/cms/attachment/201604/56fb47276f72a.jpg" alt="图1  蘑菇街的Spark生态" title="图1  蘑菇街的Spark生态" />

**图1  蘑菇街的 Spark 生态**

### 业务实践

#### Spark Streaming 的应用

Spark Streaming 作为构建在 Spark 上的流式处理框架，得益于 RDD 的抽象，很方便地就能和其他 Spark 组件进行交互。Spark Streaming 可以使用 SQL 或者 DateFrame 来处理数据，而且天然就支持时间窗口的分析。Spark Streaming 和 Storm 相比，在吞吐量上比较突出，非常方便地支持实时的粒度到分钟级、秒级，所以我们有很多 Spark Streaming 的使用场景，最主要的场景是作为 ETL 工具的一部分，做日志数据的清洗和预处理。

蘑菇街每天会有数百亿条日志数据产生，这些数据需要采集、清洗到 HDFS、HBase 等存储系统供不同的业务方进行数据分析使用。日志清洗的逻辑一般都比较简单，但是数据量比较大，对于系统的稳定性要求比较高，所以蘑菇街数据平台的日志清洗均采用相对成熟、稳定、可扩展的技术。日志处理系统流程如图2。日志分为两路，一路是离线的，通过 Camus 采集到 HDFS，通过 MR 任务清洗后写入到 Hive 中，一路通过 Spark Streaming 清洗之后写入到 HBase 中。

<img src="http://ipad-cms.csdn.net/cms/attachment/201604/56fb67de158df.jpg" alt="图2  日志处理系统框图" title="图2  日志处理系统框图" />

**图2  日志处理系统框图**

历史上所有的日志都是通过离线这一条路径采集，通过 Camus 框架定时从 Kafka 采集日志到 HDFS 中，再通过 MR 任务清洗之后存储到 Hive。随着业务对于数据查询速度的要求越来越高，我们已经或正在测试通过 Presto、Spark SQL 和 Hive on Tez 等框架来提升查询处理效率。随着业务数据量的飞速增长，对数据的实时性要求提高，我们引入了 Spark Streaming 来进行实时业务链路的日志清洗，存储到 HBase 中，供实时性要求高的业务使用，在可靠性第一，实时性第二的离线场合，还是依托原有的成熟链路，但不排除将来两条链路整合的可能性。

Spark Streaming 也存在很多问题，我们在生产实践中最关注的问题主要是 Spark 流控的问题。由于 Spark Receiver 收到的数据都是先存在内存中的，如果后续计算的处理速度比 Receiver 接收数据的速度慢，累积下来就会造成内存溢出，导致任务失败，影响了 Spark Streaming 的稳定性。在生产环境中部署会造成很大的麻烦。所以我们一直跟踪 Spark 流控的问题，Spark Streaming 从没有流控，到有静态的流控，再到动态流控，极大增强了系统的稳定性。

Spark 的静态流控比较简单，就是在数据接收端（Receiver）收到数据往 Buffer 存储的时候，通过令牌桶机制控制写入的速度。数据写入 Buffer 之前，需要先获得令牌。通过控制每秒中发放的令牌数目，就可以控制 Spark Streaming 的流速。

但是静态流控有一个弊端，如果令牌数设置过大，达不到限流的效果，如果设置的过小，浪费了系统的资源。而如果能根据系统的处理速度动态调整限流的阈值，就可以在尽可能充分利用集群计算能力的同时，保证系统的稳定性。

动态流控的实现很巧妙，通过增加流速控制器，根据系统处理的数据量，处理的延迟，来进行流速预测，预测值作为每秒钟发放的令牌数。
Spark 的动态流控目前是通过 PID （比例-积分-微分控制策略）算法来实现流速估算的。PID 是控制器领域非常基础的控制算法，根据系统的历史处理情况来调整系统的处理速度，提高系统的稳定性和效率。

在 Spark Steaming 流控的问题上，我们也做了一些自己的工作，提交到社区（https://github.com/apache/spark/pull/9593#issuecomment-159793849）。

#### Spark GraphX 的应用

GraphX 是 Spark 的图计算组件，提供了丰富的图运算功能和算法，可以轻松的在海量数据上运行复杂的图算法。GraphX 提供了一整套图算法包，开发图的应用非常方便。

在数据分析中，有很多场景使用传统的分析方法是搞不定的。比如在数据内部存在较高关联度的情况下，引入大量的 join、Agregation 操作，会导致大量数据计算和迁移，效率低下，而在一些特定的应用场景中，通过引入图计算，可以规避这些问题。接下来，介绍两个应用图计算的场景：第一个是短文本相关性计算，第二个是用户链路分析。这两个场景中，短文本相关性涉及到矩阵运算，用户链路分析则涉及到把一个链路上的多个节点串连起来，在传统的分析方法中，计算代价都很高，通过 GraphX，则可以以较小的代价实现。

**GraphX Case 1：短文本相关性，基于 simrank++的共同点击**

在搜索引擎中，用户的点击行为往往可以帮忙修正搜索引擎策略，提高搜索准确率，比如：用户在夏天搜“上衣”：倾向于点击“T 恤”，“短衬衫”等，但是在冬天，则倾向于点击“羽绒服”，“棉衣”等，所以在不同季节对“上衣”这个词需要进行不同词的关联，一方面解决“上衣”这个词本身用文本直接关联召回、商品量不足的问题，另外一方面更好的理解用户的搜索意图，提高用户点击率 。

<img src="http://ipad-cms.csdn.net/cms/attachment/201604/56fb67ea9ecdc.jpg" alt="图3  用户行为Graph" title="图3  用户行为Graph" />

**图3  用户行为 Graph**

短文本相关系分析，主要基于搜索词（query）和商品（item）构建二步图，通过 Spark 的 GraphX 进行计算，对模型的二步图经过多次迭代之后，得到一个充分扩展、边关联的 graph，以此可以得到搜索词到搜索词的相似性扩展和商品到商品的相似性等 ：

>理论基础：

>**http://ilpubs.stanford.edu:8090/870/1/2008-17.pdf**

>训练数据：

>**query 4w+**, 商品 **50w+** ，原始 **(query, item) pair** 约**300w**

>中间数据量：

>**item2item**数量：

>**walk 1：2**亿，占用内存约 **6G**

>**walk 2：32** 亿，占用内存约 **74G**

>**walk 3：50** 亿，占用内存约 **150G**

一些实践经验：

- 避免单个 query 结点映射到过多的 item 结点，无论从关联的实际效果出发还是从防止数据倾斜的角度出发，都应该根据 query 到 item 的出度对转移矩阵进行剪枝。

- 避免 flatMap 相关的暴力操作，很容易报内存不足错误:

- 避免数据直接 join 操作，避免大数据 broadcast，如果是倾斜的数据，可以以分区为单位，分别对分区进行 cartisian

- 如果数据量大，spark.storage.memoryFraction 的值设的尽量小，数据 store 到磁盘上，用时间换空间。

**GraphX Case 2：订单来源追踪**

电商网站每天的流量都很大，用户在进入网站进行购物的过程中，会有多次的浏览和点击数据生成。通过分析用户进入网站或者 App，直到下单购买的轨迹，对于网站的运营和营销活动的效果评估都具有非常重要的意义。

用户的点击数据量是非常大的，从用户的行为数据复原用户的浏览轨迹，传统的方式需要大量的数据 join，但是有的用户浏览轨迹会非常长，进行 N 跳的分析，就需要 N 次 Join,N的值比较大的情况下，计算非常缓慢，代价也很大，所以我们选用了 GraphX 来做这件事情。具体的做法如下：

Step 1: 把用户的一次浏览点击行为抽象成 graph 的顶点，通过每次行为的上下文衔接信息构造边，构建一个大图。

Step 2: 把用户发生购买行为的点标记为起点，把业务方比较关注的结点作为终点，从起点开始发送消息，当一个消息传递到终点的时候，就停止传递，最后统计终点收到的信息，就可以得到用户从哪些结点进去之后最终产生了成交。在消息的传递过程中，如果每个消息记录自己经过的顶点，则可以还原出一笔订单产生时整个用户的浏览轨迹。消息传递采用 Pregel 模型来实现。

通过 GraphX 提供的算子，可以很容易的实现整个操作。GraphX 提供的计算能力，使得之前由于计算资源限制，没法实现的模型和业务，都能得到有效开展。

此外，目前我们也在基于 GraphX 尝试社区分析，用户影响力传播等方面的工作。


#### Spark MLlib 的应用

MLlib 是 Spark 提供的机器学习组件，提供了丰富的学习算法和工具，包括分类、回归、聚类、协同过滤、降维。Spark MLlib 在蘑菇街有很多应用场景，如排序、搜索、反作弊、用户画像等。下面简单介绍两个应用场景：第一个是在图墙排序中，利用 Spark 做进行 LTR 图墙排序；第二个是反作弊团队基于 Spark 构建的反作弊数据处理框架。

**MLlib Case 1: LTR 的图墙排序**

作为社会化的电商，不同于传统的电商网站，蘑菇街的图墙排序具有自身的特点。因为用户进入蘑菇街之后，很多时候是在“逛”，而不是直接通过搜索进入购物行为，所以在用户进入页面后，图墙的数据怎样排序，直接影响到用户的体验，影响网站的成交额，综合很多因素之后，我们选择了基于 LTR 的排序方案。相比较其它方案而言，LTR 具备多个方面的优势：融合更多商品信息、用户信息、场景信息；降低错误识别的风险；通过数据驱动，自动发现合适的排序方式，减轻调参的工作量；具有可复制性，使得不同场景和不同目标的模型迁移更新变得容易；支持适度的可解释性和人工干预。

LTR 是一种有监督的学习，主要流程包括：样本标注、特征提取、模型训练、预测。样本标注最开始采用的商品维度的样本标注，使用了 Spark 后，由于数据处理能力变强，所以采用了日志维度的标注。采用日志维度的样本标注之后，大量的日志，数据使得抽取的特征更为丰富，得到更好的效果。日志标注流程如图4所示。

<img src="http://ipad-cms.csdn.net/cms/attachment/201604/56fb68c1de2c5.jpg" alt="图4  日志标注流程图" title="图4  日志标注流程图" />

**图4  日志标注流程图**

商品的本质还是数据，不同的商品在价格、材料、类目、商家等多个维度有不同的属性，根据这些属性抽取出数据特征，进行建模，模型训练的流程如图5所示。

<img src="http://ipad-cms.csdn.net/cms/attachment/201604/56fb68f055ed2.jpg" alt="   图5  LTR排序模型训练流程" title="   图5  LTR排序模型训练流程" />

**图5  LTR 排序模型训练流程**

**MLlib Case 2: Spark 在反作弊中的应用**

在电商网站中，商品的排名至关重要，影响商品的销量和用户的购物体验。有些商家为了提高排名，会采取一些非常规的手段。这些作弊手段严重损害了买家的利益，对于其他商家也是不公平的。所以必须通过技术手段，来识别作弊用户和行为，保证用户的利益。由于用户作弊的行为比较复杂，需要一些复杂的算法和建模，Spark MLlib 提供了非常丰富的机器学习算法实现，Spark Streaming 可以非常方便地进行日志处理，所以公司反作弊团队基于 Spark 构建了整个流程。公司反作弊系统架构如图6，具体流程如下：

<img src="http://ipad-cms.csdn.net/cms/attachment/201604/56fb6925a9a47.jpg" alt="图6  反作弊系统架构图" title="图6  反作弊系统架构图" />

**图6  反作弊系统架构图**

- 在数据采集阶段，利用 Spark Streaming 完成行为数据采集与清洗。

- 在模型学习阶段，利用 Spark MLlib 和 Spark ML Pipeline 构建基于决策树的机器学习模型。

- 在构建基于机器学习模型服务阶段，利用 prediction IO 构建基于 Spark 的机器学习模型引擎，稳定的输出预测概率结果。

- 在数据可视化阶段，利用 pyspark 快速的获取结构化分析结果。

### 蘑菇街 Spark 开发平台介绍 

随着蘑菇街公司内部使用 Spark 的业务团队越来越多，通过 Spark 运行的任务数和数据量都在增大。但是并不是每个使用 Spark 的业务方都非常熟悉Spark编程，为了推动业务快速应用和部署 Spark Applicaiton，我们开始构建 Spark 开发管理平台。目前该平台主要有3个组件：1. 任务调度，主要负责 Spark 任务的管理调度和生产环境下的部署。2. NoteBook，作为用户的开发调测工具，便于数据分析人员开发和调试相关应用。3. 专家系统，收集 Spark 应用在运行时的日志数据，根据制定的规则自动分析用户的程序，检测用户的应用模式是否合理，帮助用户定位问题和性能调优。

<img src="http://ipad-cms.csdn.net/cms/attachment/201604/56fb698837ca2.jpg" alt="图7  蘑菇街Spark开发平台架构" title="图7  蘑菇街Spark开发平台架构" />

**图7  蘑菇街 Spark 开发平台架构**

开始使用 Spark 的时候，各个业务方都是通过命令行自己提交任务，管理比较混乱，也不支持任务依赖的管理，所以我们开发了任务调度系统。让用户通过调度界面配置自己的任务信息，上传依赖文件，调度系统就会自动生成任务，触发提交任务执行引擎执行。现在支持的调度方式有固定周期、固定间隔、依赖触发调度等。下一步的目标是支持 NoteBook 任务的调度，用户通过 NoteBook 编辑调试的任务，直接配置调度信息，纳入调度系统管辖。

Spark NoteBook 主要是帮助数据分析人员、机器学习工程师能够方便的进行交互式的任务调试。目前我们基于 Jupyter + Toree 搭建了 NoteBook 开发环境。下一步我们的目标是把 NoteBook 环境进行虚拟化，用户在申请工作空间后，自动获得 NoteBook 开发环境。

Spark 专家系统主要是针对于业务开发人员进行问题诊断和性能优化。通过收集 Spark 作业运行时的 Metric 打点信息，进行分析汇总，并制定一些规则来分析 Spark 作业的问题，比如分区不合理、数据倾斜、Shuffle 太多等。下一步我们的目标是根据统计需求，定制一些
 Metric 打点信息，逐步增加更复杂的分析规则。

Spark 作为一种强大的海量数据处理引擎，给数据分析带来了很大的便利，但是对于没有相关知识背景的数据分析人员，使用门槛依然较高。Spark 在蘑菇街的应用才刚刚开始，未来我们将投入更大的精力，进一步提升 Spark 开发管理平台的服务能力，降低用户使用成本，让 Spark 在蘑菇街走得更快，走得更远。
## 360云查杀服务从零到千亿级 PV 的核心架构变迁

文 / 魏自立

本文介绍了360云查杀服务核心在线引擎的架构变迁过程，经历了从无到有，再到后来每天处理千亿级 PV 的海量并发网络服务。

### 云查杀服务介绍
几年前，传统杀毒软件的病毒查杀基于本地特征库实现，查杀新的病毒木马依赖于升级特征库。 随着升级，特征库会越来越大，查杀能力的滞后现象也非常严重。对此360在2009年推出了云查杀系统。

图1简要说明了云查杀系统几个最为核心的功能模块。

<img src="http://ipad-cms.csdn.net/cms/attachment/201608/579ef4e283ff7.png" alt="图1 云查杀系统核心功能模块图" title="图1 云查杀系统核心功能模块图" />

图1 云查杀系统核心功能模块图

当用户在电脑上运行一个软件（很可能是个恶意程序）时，360卫士会及时将该软件的 MD5（备注：MD5冲突是2014年集中爆发的，为此我们增加了 SHA1的计算和判断）值计算出来，并发给云查杀服务器，云查杀服务器就在样本黑白库中查找该MD5对应的黑白级别，如果找到就返回相应级别；如果没有找到就返回未知，并告诉客户端让其将该样本上传。客户端收到服务器返回的级别信息后，及时做出是否拦截的策略，如果需要上传，就将该样本通过上传接口上传到服务器。服务器接收到该样本后，及时进行样本扫描和鉴定，将鉴定结果再推送到云端样本黑白库中，这样下一个用户再遇到这个样本时，就能立刻查杀。

### 云查杀服务核心在线引擎的架构变迁
#### 第1代架构：单机
那是2009年，云查杀系统第一次上线，360云端样本收集量还不是很大，样本黑白库只用一个单机版的 memcached 就完全存下了。 所有样本黑白信息库都存储在 memcached 的内存中。服务器业务逻辑主要是在一个 Nginx 的 HTTP 扩展模块中实现的。这一阶段的核心目标是实现云端病毒查杀能力，因此这个方案是最简单、最容易实现的。

单机版云查杀在线引擎架构如图2所示。
<img src="http://ipad-cms.csdn.net/cms/attachment/201608/579ef73c6c29c.png" alt="图2 单机版云查杀引擎架构图" title="图2 单机版云查杀引擎架构图" />

图2 单机版云查杀引擎架构图

#### 第2代架构：UDP
随着360卫士功能不断丰富以及用户量不断增长，第1代架构下的 HTTP 请求给服务器带来很大的性能压力。分析发现，大部分请求的数据报文的大小都在1000字节左右，当时的思路就是能否用 UDP 协议代替 HTTP 协议来传输报文？

经过调研测试，同等条件下，UDPServer 的 QPS 是 HTTPServer（Nginx）的数倍，因为 HTTP 协议是建立在 TCP 协议之上的文本协议（备注：当时是 HTTP1.1），文本解析有一定的开销，TCP 连接建立有三次握手、连接断开有四个挥手，这些开销都不小，进而影响整体服务的 QPS，因此我们认为使用 UDP 作为传输协议应该能缓解当时的服务器性能压力。

但是 UDP 协议的一个致命缺陷是不太可靠，数据包有可能会丢失，极端情况下甚至会出现100%丢包。为了解决这个问题，我们用 HTTP 协议做备用。在客户端发送 UDP 报文到服务器后，如果一定时间内没有收到服务器的回应，就认为该次 UDP 请求失败，然后立刻用 HTTP 重试。

这一代架构虽然解决了服务器性能瓶颈，但引入了另一个问题，就是 HTTP 和 UDP 服务虽然是同一套业务逻辑，但用了两套几乎完全独立的代码实现的。这是由于 HTTP 服务是基于 Nginx 的 HTTP 扩展模块实现的，严重依赖于 Nginx 框架及其数据结构，例如 ngx_pool_t、ngx_array_t 等，这些代码没法一次性地全部移植到 UDPServer中。 当时为了尽快解决生产环境的服务器性能压力问题，只能大干快上，完全独立开发了一套代码逻辑以适应于 UDPServer。

这其实就是我们常说的技术债务。一般而言，技术债务的产生，都有一些客观的因素，例如项目进度太紧，来不及全方位考虑和实施。本次架构变更是由于服务器性能跟不上了，公司扩容新的服务器一方面要考虑成本，另一方面时间也很可能来不及，因此只能这么临时性地解决，进而导致技术债务的产生。

增加 UDPServer 之后的云查杀在线引擎架构如图3所示。

<img src="http://ipad-cms.csdn.net/cms/attachment/201608/579ef55a621f5.png" alt="图3 第二代云查杀引擎架构图" title="图3 第二代云查杀引擎架构图" />

图3 第二代云查杀引擎架构图

#### 第3代架构：memcached 集群
经过近1年的发展，360收集的样本越来越多，单机 memcached 的内存占用很快就达到物理内存的80%警戒值了，必须对存储容量进行扩容。

首先想到的办法就是使用更大物理内存的机器，但无法根本地解决问题，单机内存再大，也总有用完的那一天。为了彻底解决问题，只能采用分布式存储架构。

当时，一个基于 memcached 开发的名为 membase 的项目进入了我们的视野。经过周密的测试，membase 表现相当不错，而且完全兼容 memcached 协议，这样迁移起来代价很小。 membase 集群由一组 memcached 组成，例如当时我们是按照5个 memcached 节点组成一个集群。membase 集群通过一致性哈希算法将数据划分到1024个 bucket 中， 这些 bucket 再被均匀分布到集群内的 memcahed 节点。集群还可以做主备模式，当其中一个 memcached 挂掉之后，备份节点可以直接顶上去使用。

存储集群化改造后的云查杀在线引擎架构如图4所示。

<img src="http://ipad-cms.csdn.net/cms/attachment/201608/579ef571d0eb5.png" alt="图4  存储集群化改造后的第三代云查杀引擎架构图" title="图4  存储集群化改造后的第三代云查杀引擎架构图" />

图4  存储集群化改造后的第三代云查杀引擎架构图

#### 第4代架构：统一 HTTP、UDP 两套代码
经过第3代架构的 memcached 集群化改造之后，系统各个环节的瓶颈暂时都没有了。在项目压力稍稍轻松一点的时候，偿还项目中遗留的技术债务就显得尤为重要。技术债务越早偿还对后续的影响就越小。事实上，我们这个债务已经拖了1年有余，在此期间的新需求开发都需要在 HTTPServer（Nginx 扩展模块）和 UDPServer 两个项目中分别开发。

我们决定将两份代码合二为一。实施过程中，有以下几个问题需要解决：
- HTTPServer 代码是基于Nginx写的扩展模块，上文介绍过，其代码逻辑与 Nginx 本身耦合较紧密，依赖 ngx_pool_t、ngx_array_t、ngx_hash_t 等数据结构；
- HTTPServer 的应用层报文协议依赖 HTTP Header 上从两个参数，并且使用 Multi-Part 方式的 HTTP POST 协议；
- UDPServer 是单包的文本协议，INI 格式；
- UDPServer 相对来说，没有了 Nginx 的内存池技术，更容易出现内存泄露等 Bug，部分代码逻辑就直接在函数内部申请内存后并返回了其内存指针，这给外层调用者带来一定负担（时刻记得要释放该内存）。

要使两套协议统一，就不能依赖 HTTP 协议特有的特性，例如 HTTP Header、Multi-Post 等，基于此，应用层协议就选择用 UDP 服务器的 INI 文本协议来实现。但又因为 UDPServer 的代码健壮性不如 HTTP 代码，我们又得从功能、代码健壮等方面在两套代码中进行平衡和选择。如何解决这个问题呢？下面从几个方面进行介绍。
##### **Nginx模块通用框架**
我们当时有很多业务，其 API 接口都是使用 Nginx 扩展模块的方式开发实现的，业务代码与 Nginx 紧耦合，最为关键的是，每一位业务开发人员因此都需要熟悉 Nginx 模块开发。在最开始阶段开发人员不多的时候，项目能很快上线，性能也有保障，但到后来也引入一些不好的因素：
- 很多同质化的代码充斥在各个项目中；
- 每次新开一个项目就是 Copy 一份代码然后稍加修改以适应新项目的需求；
- 如果有 Bug 修复或通用性的新特性开发出来，就需要在所有的项目中修改；
- 容易出错；
- 代码不容易维护；
- 让每一位开发同学都去熟悉和精通 Nginx 模块开发并不是一件轻松的事情。

为此，我们将 Nginx 扩展模块改造成一个通用的 HTTP Module（称之为 ngx_http_qihoo_unis_module），该 Nginx 模块通过 dlopen 方式打开一个业务逻辑模块（称之为 module.so），实现与 Nginx 完全解耦，与通信协议（HTTP）无关，业务全部封装在 module.so 内部。这样一来，Nginx 就只负责处理 HTTP 网络请求，module.so 就只关心其自己的业务逻辑。

为了便于叙述，简化的 module.so 原型定义如下：
```
struct ngx_pool_s;
class Module
{
  public:
    virtual bool Initialize(const char* confpath) = 0;
    virtual void Uninitialize() = 0;
    virtual int Process(struct ngx_pool_s* pool,
      const void* inputdata, size_t len,
      std::string& result) = 0;
};              
 
#ifdef __cplusplus
extern "C" {
#endif
    void* CreateModule();
    void  DestroyModule(void* m);
#ifdef __cplusplus
}//extern "C"
#endif
```
代码1

Nginx 启动阶段初始化逻辑如下：
- 进入 ngx_http_qihoo_unis_module 的初始化代码中；
- 通过 dlopen 方式将 module.so 加载进来；
- 调用其中的CreateModule函数，创建一个具体的 Module 对象m；
- 调用 m.Initialize 初始化业务逻辑模块。
Nginx 初始化完成之后，当接收到 HTTP 网络请求时，相应地会进入 ngx_http_qihoo_unis_module 逻辑中，我们就将 HTTP 请求的 Body 数据全部读取出来，调用 m.Process 函数进而直接进入业务层代码。业务层逻辑处理完毕之后，通过最后的那个输出参数 result 直接将结果返回给 Nginx 框架层。
##### **UDPServer改进**
上述 Nginx 通用模块改进之后，已经抽象出统一的逻辑层接口，因此将 UDPServer 也进行抽象，其业务处理逻辑也完全交给 module.so 处理。
##### **业务核心逻辑重构**
前文讨论过，UDPServer 的业务逻辑代码相对来说比 HTTPServer 的健壮性性要差些。因此我们业务逻辑的重构还是以 HTTPServer 的代码为基准，也就是以原来的 Nginx 扩展模块里的逻辑代码为基准。为了快速迭代，直接将 ngx_pool_t 这个数据结构及其相应操作函数直接全部移植出来，以 qh_前缀统一命名，例如 qh_pool_t，并用宏定义来实现与 ngx_pool_t 等变量和函数的等价替换。这么一来，业务逻辑使用 ngx_pool_t 的地方，就不用修改任何代码了。其他的 Nginx 相关数据结构则都用相应的 STL 库替换。
##### **测试**
经过上述三个大的改进之后，逻辑代码统一了，并积累了两个进程框架：基于 Nginx 的 HTTP 扩展框架和 UDPServer 框架。接下来就是功能、稳定性、性能等各种测试了。当时，我们没有专门的服务端 QA 团队，这些测试工作只能由开发人员自己完成。

功能测试，采用的是数据包重放黑盒对比的方式进行。我们将生产环境的数据包直接镜像一份出来，包括 HTTP 和 UDP 之后，再将这些数以百万计的数据包分别发往原来的生产环境服务器和新重构完成的服务器，然后比对双方的返回结果，如果完全一致，则认为重构的逻辑没有问题。

性能测试，其实就是压测，对比新老服务的 QPS 差距，用 pprof 等工具进行性能优化。当时印象特别深刻的一点就是字符串拷贝占用 CPU 时间比较多，对比分析发现，老程序都是基于 C 语言的字符串操作，重构之后的程序大量用到 STL 标准库里的 std::string 字符串类，于是我们将所有类似于 std::string product = 
"product"; 之类的常量字符串赋值操作都进行了如下改进：
```
cpp static const std::string _s_product = "product"; std::string product = _s_product;
```

如此改进，就将字符串拷贝动作只在静态变量第一次初始化的时候进行一次拷贝，然后在运行过程中，利用 std::string 的 Copy-On-Write 技术，一般情况下并不是真的拷贝而只是引用技术的增减。 仅此一项改进性能就提升30%多。

稳定性测试比较容易，我们仍然采用流量镜像的方式，将生产环境的流量原封不动地导入进来测试。

两套代码统一之后的第4代架构最终如图5所示。

<img src="http://ipad-cms.csdn.net/cms/attachment/201608/579ef5835dead.png" alt="图5  统一HTTP、UDP两套代码后的云查杀引擎架构图" title="图5  统一HTTP、UDP两套代码后的云查杀引擎架构图" />

图5  统一 HTTP、UDP 两套代码后的云查杀引擎架构图

### 第5代架构：memcached 集群的持久化
虽然前面解决了很多很多问题，但随着公司的发展，我们又面临更多的问题，主要集中在下面几点：
- 前文介绍我们已经对样本黑白信息库的存储进行了集群化的改造，但360搜集的样本仍然在快速增长，总有一天当前的这个5台服务器组成的集群也存不下全部样本黑白信息库。
- memcached 的数据都在内存中存放，一旦死机后再进行数据恢复，就得从其他对等机器上拷贝数据。导致恢复时间较长且较为繁琐；
- UDP 丢包后 HTTP 重试机制也有缺陷，比如某些时候 UDP 超时严重（例如网络拥塞、服务器处理不过来等因素），切换到 HTTP 重试，又进一步加剧服务器负担，而引起雪崩现象，全部服务都不可用。

为了解决这些问题，我们分两步进行架构重构。

将 memcached 的数据持久化到 SSD，是一个比较好的思路。一方面相对长时间地解决了样本增长而内存容量有限的问题；另一方面，在死机后的数据恢复方面比较有利，因为数据就在本地 memcached 直接启动就好了，根本不需恢复。

但 SSD 相对于内存而言，其访问速度下降了几个数量级，是否能够满足我们的时延要求还是未知数。为此，我们针对云查杀日志进行统计分析，发现90%以上的请求都集中在流行的白名单样本上面，这给了我们一个启示：如果将这部分数据 cache 在内存中，那么整体性能应该不会下降太多。实际上就是将热数据 cache 在内存中，冷数据放在 SSD 中。

memcached 持久化改造过程中，我们引入了三个组件（概念）：
- leveldb，Google开发的一款高性能的持久化 Key-value 存储库。我们用来将 Key-value 数据存储在 SSD 上；
- LRU，是 Least Recently Used 缩写，近期最少使用算法。我们用来实现冷数据从内存中的自动淘汰策略；
- BloomFilter，是由 Howard Bloom 在1970年提出的二进制向量数据结构，它具有很好的空间和时间效率，被用来检测一个元素是不是集合中的一个成员。我们用在这里来解决一个 Key 是否在 leveldb 存储中，以减少对 SSD 磁盘的读操作。

当一个写操作到来时，改造后的 memcached 内部逻辑依次为：
- 写内存中的 LRU 队列；
- 写 BloomFilter；
- 写 leveldb；
- 上述全部成功后再返回给客户端表明写成功。

当一个读操作到来时，其内部逻辑为：
- 先读 LRU 队列，如果找到就直接返回；
- 否则再读 BloomFilter，如果找到就表明该 key 很可能在 leveldb 中（这里有一定的错误概率，由 BloomFilter 的参数决定，一般可以设置为小于十万分之一）；
- 继续读 leveldb，如果读取到了，就将数据写入 LRU 队列中，最后再返回给客户端。

为了进一步降低时延，我们将 moxi 这个中间层代理组件去掉了，将一致性哈希算法和 Failover 策略完全实现在客户端库中。该版本上线后，生产环境实测数据表明，memcached 的10个 key 的 mget 操作，平均时延在0.17ms左右。

memcached 的持久化改造解决了上述几个问题中前两个，最后一个问题留给下一代架构解决，整体架构如图6所示。

<img src="http://ipad-cms.csdn.net/cms/attachment/201608/579ef593e7593.png" alt="图6  存储持久化的云查杀引擎架构图" title="图6  存储持久化的云查杀引擎架构图" />

图6  存储持久化的云查杀引擎架构图
#### 第6代架构：TCP
memcached 持久化改造完毕之后，我们接着又开始对网络层进行改进，希望解决第5代架构中遗留的那个问题。这一次我们引入 TCP 长连接机制，既保证消息传输的可靠性，又减少 TCP 连接阶段三次握手带来的时延和开销，同时还提供服务端主动推送消息的能力。

一开始我们就考虑到了多业务的场景下，客户端只需要与服务器端建立一条连接，在这条连接上可以传输不同的业务数据包消息。我们引入 Gateway 组件与客户端保持连接，设计上至少达到单机承载100万以上的 TCP 长连接。Gateway 通过 ZooKeeper 实时感知 Backend 服务器所在，然后用 TCP 长连接连过去，业务消息过来时就转发到 Backend 上。

这一次改进上线后，客户端打点统计表明，1秒内就完成网络请求和响应的比例从60%提升到86%，效果提升非常明显。

增加 TCP 长连接之后的第六代架构如图7所示。

<img src="http://ipad-cms.csdn.net/cms/attachment/201608/579ef5a511ae6.png" alt="图7  支持TCP长连接的云查杀引擎架构图" title="图7  支持TCP长连接的云查杀引擎架构图" />

图7  支持TCP长连接的云查杀引擎架构图

#### 第7代架构：Trident 三协议合一架构
随着 TCP 长连接服务的上线，新的问题又出现来。

以前只有 HTTPServer、UDPServer 时，这两个程序都是多进程框架，类似于 Nginx 的 master-workers 工作模式，当有字典数据更新时，直接 Reload 整个进程框架就可以解决。现在上线 TCP 服务之后，沿用之前的进程框架，整体上依旧是多进程框架，但是由于 TCPServer 与 Gateway 直接还保持这一条长连接，在 Reload 过程中，还有好多请求没有处理完毕就重启了，导致丢请求现象严重。

另外，字典文件的更新频率也越来越高，有些时候，甚至达到1分钟1次的频率，这导致线上服务频繁地需要 Reload。每次 Reload 的时候，都是 HTTP/UDP/TCP 三个服务一起，在这一过程中，服务器 CPU 资源特别紧张，经常过载，也会导致请求处理不过来而丢请求。

三个进程一起 Reload，能否合并到一个进程，这样只需要 Reload 一次就好。另外，每次 Reload 时，其实只是一个字典文件更新，但由于是进程 Reload 会导致所有的数据全部都得重新加载，能否按需加载？ 假设我们的服务依赖10个资源数据文件，其中一个资源数据更新，我们只需要重新加载这个资源的新数据就好，而其他的资源由于不需要更新，就继续沿用之前的数据。

解决上述两个小问题，就可以解决在数据加载时 Reload 进程时 CPU 负载高进而导致丢请求的问题。

第一个问题的解决，需要重新开发一个网络框架，直接提供三种协议的接入服务：HTTP、TCP、UDP。

解决第二个问题，需要有一种机制，能够更新进程内某个数据结构对象，而不影响其他对象。 这在多进程框架下是比较难以做到的，因为这就相当于要将多个 worker 进程内相关的资源数据结构都放到共享内存中。这些数据结构一般都是自定义类型，例如 map、list 等各种组合结构，这个对编程人员的水平要求太高而且不易维护。那么，就只能将多进程框架改为多线程框架。因为多线程框架下，对进程内的数据结构更新，所有线程都可以看见。

为此，我们针对所有线程不安全的代码都进行重构。当然，这个重构过程也是有讲究的，持续时间较长，每重构一两个函数，就做一次回归测试和上线，确保小粒度的重构和上线所带来的影响范围控制到最小，这样即使重构之后的代码逻辑不符合预期，我们也比较容易知道是什么地方出了问题。如果一次性全部重构完成之后，再上线，很可能出了问题都不知道在什么地方。

当所有线程不安全的代码都重构完成之后，并且功能正常，我们就将程序切换到多线程模式运行。 然后再使用双缓冲 DoubleBuffering 机制解决资源数据按需加载的问题。

至此，我们的 HTTP/TCP/UDP 三协议合一的框架就形成了，我们在公司内部比较形象的将其命名为三叉戟 Trident，整体框架如图8所示。

<img src="http://ipad-cms.csdn.net/cms/attachment/201608/579ef5bb1728f.png" alt="图8  三协议合一的云查杀引擎架构图" title="图8  三协议合一的云查杀引擎架构图" />

图8  三协议合一的云查杀引擎架构图

### 总结
架构对一个系统而言，无论什么阶段都很重要。但并不是每个阶段的架构都必须是完美的才能解决问题，因为世界上并没有完美的东西，也就没有完美的架构。

当程序员面临实际的项目需求时，解决当前最重要最紧急的任务其实就是架构面临的核心问题。随着项目的演进、时间的推移，原来还能工作的架构也许运行的得不那么好了，这个时候就需要进行架构方面的重构。这个架构重构，有时候很大，需要涉及到很多组件，例如我们经历的第6次架构，增加 TCP 长连接的架构变迁；有时候架构调整又很小，例如我们经历的第5次重构，仅仅是程序内部的逻辑重构和改进。

我们不能说上述架构已经完美，但目前阶段的确满足了360公司对云安全系统的业务要求。我们也不能说上述每一次架构变迁都是正确的选择或者说都是最优的解决方案，但那都已经成为过去，我们记录于此，只是对我们自己走过的路进行回顾和总结。同时为读者提供一个思路，一个特定条件下行得通的思路，如果读者能顺便从中获得点什么启发，那我们就非常满足了。